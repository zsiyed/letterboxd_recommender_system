{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65efcca3-c18f-4e92-807b-a57796d8acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import json\n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be027de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11322\n"
     ]
    }
   ],
   "source": [
    "with open('../letterboxd_proj_data/failed_fetches.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "print(len(json_list))\n",
    "# for json_str in json_list[:10]:\n",
    "#     result = json.loads(json_str)\n",
    "#     print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7cfc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182284\n"
     ]
    }
   ],
   "source": [
    "with open('../letterboxd_proj_data/usernames_sample_100.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "print(len(json_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "838a63f0-4231-4c65-99df-4dbb60d8d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f986115-2240-4fb4-ac7d-89ad2e0381ea",
   "metadata": {},
   "source": [
    "- 30/35 responses per page, popular reviews section double counts the top 5 people\n",
    "- 1.5-2s per response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674a888-aed2-4b92-8f1d-c479bbdbbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_fetch = 0\n",
    "# usernames = set()\n",
    "# for i in range(1):\n",
    "#     url = \"https://letterboxd.com/members/popular/this/year/page/{}/\".format(i)\n",
    "\n",
    "#     # Fetch the webpage\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     # Check if request was successful\n",
    "#     if response.status_code == 200:\n",
    "#         # Parse HTML\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#         user_elements = soup.select(\".person-summary .name\")\n",
    "\n",
    "#         # Extract and print usernames\n",
    "#         for user in user_elements:\n",
    "#             usernames.add(user[\"href\"][1:-1])\n",
    "#     else:\n",
    "#         print(\"failed iter {}\", i)\n",
    "#         failed_fetch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../letterboxd_proj_data/kaggle_data/usernames.json', 'r') as file:\n",
    "    try:\n",
    "        usernames_json = json.load(file)\n",
    "    except json.JSONDecodeError:\n",
    "                usernames_json = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d9f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_sample = usernames_json['usernames'][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f6241",
   "metadata": {},
   "source": [
    "## Scrape review data\n",
    "- scraping 1 users reviews took roughly 1 min\n",
    "- url doesnt work past 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667c5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_review_page(user, i, session, output_file_name):\n",
    "    url = \"https://letterboxd.com/{}/films/reviews/page/{}\".format(user, i)\n",
    "    response = session.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find all film detail blocks\n",
    "        film_blocks = soup.find_all(\"div\", class_=\"film-detail-content\")\n",
    "        # print(user, url)\n",
    "        # Iterate over each block and extract the required information\n",
    "        for film in film_blocks:\n",
    "            # Extract film title\n",
    "            film_title_tag = film.find(\"h2\", class_=\"headline-2 prettify\").find(\"a\")\n",
    "            film_title = film_title_tag.text.strip() if film_title_tag else \"Unknown Title\"\n",
    "            # print(film_title)\n",
    "\n",
    "\n",
    "            # Extract star rating\n",
    "            rating_tag = film.find(\"span\", class_=\"rating\")\n",
    "            rating_text = rating_tag.text.strip() if rating_tag else None\n",
    "            if rating_text:\n",
    "                rating = rating_text.count('\\u00BD')*0.5 + rating_text.count('\\u2605')\n",
    "\n",
    "            # Extract date\n",
    "            date_tag = film.find(\"span\", class_=\"_nobr\")\n",
    "            date_string = date_tag.text.strip() if date_tag else None\n",
    "            if date_string:\n",
    "                date_object = datetime.strptime(date_string, \"%d %b %Y\")\n",
    "                unix_timestamp = int(date_object.timestamp())\n",
    "            else:\n",
    "                print(\"no date \", user, film_title, date_tag)\n",
    "\n",
    "            # Check if \"icon-liked\" exists in the attribution block\n",
    "            liked_icon = film.find(\"span\", class_=\"has-icon icon-16 icon-liked\")\n",
    "            liked = 1 if liked_icon else 0\n",
    "\n",
    "            # Extract review text\n",
    "            review_body_tag = film.find(\"div\", class_=\"body-text -prose js-review-body js-collapsible-text\")\n",
    "            review_text = review_body_tag.get_text(\" \", strip=True) if review_body_tag else None\n",
    "\n",
    "            # save review to json\n",
    "            reviews_file_name = \"../letterboxd_proj_data/{}.json\".format(output_file_name)\n",
    "            save_review_to_json(reviews_file_name, user, film_title, unix_timestamp, rating, liked, review_text)\n",
    "    else:\n",
    "        log_failed_fetch((i, user))\n",
    "    # time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "def save_review_to_json(file_path, user, film_title, unix_timestamp, rating, liked, review_text):\n",
    "    # Create a dictionary with the extracted details\n",
    "    review_data = {\n",
    "        \"user\": user,\n",
    "        \"title\": film_title,\n",
    "        \"review_date\": unix_timestamp,\n",
    "        \"rating\": rating,\n",
    "        \"liked\": liked,\n",
    "        \"review_text\": review_text\n",
    "    }\n",
    "    \n",
    "    # Load existing data if the file exists, else start with an empty list\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []  # If the file is empty or corrupted, reset to an empty list\n",
    "    else:\n",
    "        data = []\n",
    "    \n",
    "    # Append new data to the list\n",
    "    data.append(review_data)\n",
    "\n",
    "    # Save the updated data back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "def log_failed_fetch(i, user):\n",
    "    print('fail L nerd moment')\n",
    "    # Load existing data if the file exists, else start with an empty list\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []  # If the file is empty or corrupted, reset to an empty list\n",
    "    else:\n",
    "        data = []\n",
    "    \n",
    "    # Append new data to the list\n",
    "    data.append((i,user))\n",
    "\n",
    "    # Save the updated data back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809da726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(usernames, output_file_name):\n",
    "    # Create a session\n",
    "    session = requests.Session()\n",
    "    for user in usernames:\n",
    "        url = \"https://letterboxd.com/{}/films/reviews/\".format(user)\n",
    "        \n",
    "        # Fetch the webpage\n",
    "        response = session.get(url, headers=headers)\n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            last_page = max(int(a.get_text()) for a in soup.select(\".paginate-page a\") if a.get_text().isdigit())\n",
    "\n",
    "            for i in range(1, last_page + 1):\n",
    "                scrape_single_review_page(user, i, session, output_file_name)\n",
    "        else:\n",
    "            log_failed_fetch((-1, user))\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd66459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no date  deathproof 13th <span class=\"_nobr\"><time class=\"localtime-d-mmm-yyyy\" datetime=\"2020-06-05T03:36:38.905Z\"></time></span>\n"
     ]
    }
   ],
   "source": [
    "get_reviews(usernames_sample, \"usernames_sample_1\") # took 3.30 with just session improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aadf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_THREADS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba969e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reviews_multi(usernames, output_file_name):\n",
    "    # Create a session\n",
    "    session = requests.Session()\n",
    "    for user in usernames:\n",
    "        url = \"https://letterboxd.com/{}/films/reviews/\".format(user)\n",
    "        \n",
    "        # Fetch the webpage\n",
    "        response = session.get(url, headers=headers)\n",
    "        # Check if request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            last_page = max(int(a.get_text()) for a in soup.select(\".paginate-page a\") if a.get_text().isdigit())\n",
    "            threads = min(MAX_THREADS, last_page)\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(scrape_single_review_page, user, i, session, output_file_name)\n",
    "                    for i in range(1, last_page + 1)\n",
    "                ]\n",
    "                # Wait for all tasks to complete\n",
    "                concurrent.futures.wait(futures)\n",
    "\n",
    "        else:\n",
    "            log_failed_fetch((-1, user))\n",
    "    session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e2d44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no date  deathproof 13th <span class=\"_nobr\"><time class=\"localtime-d-mmm-yyyy\" datetime=\"2020-06-05T03:36:38.905Z\"></time></span>\n"
     ]
    }
   ],
   "source": [
    "get_reviews_multi(usernames_sample, \"usernames_sample_4\") # took 30s with multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9c6be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
