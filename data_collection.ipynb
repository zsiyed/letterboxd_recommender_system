{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65efcca3-c18f-4e92-807b-a57796d8acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import json\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838a63f0-4231-4c65-99df-4dbb60d8d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set headers to mimic a browser request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f986115-2240-4fb4-ac7d-89ad2e0381ea",
   "metadata": {},
   "source": [
    "- 30/35 responses per page, popular reviews section double counts the top 5 people\n",
    "- 1.5-2s per response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f674a888-aed2-4b92-8f1d-c479bbdbbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_fetch = 0\n",
    "# usernames = set()\n",
    "# for i in range(1):\n",
    "#     url = \"https://letterboxd.com/members/popular/this/year/page/{}/\".format(i)\n",
    "\n",
    "#     # Fetch the webpage\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     # Check if request was successful\n",
    "#     if response.status_code == 200:\n",
    "#         # Parse HTML\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#         user_elements = soup.select(\".person-summary .name\")\n",
    "\n",
    "#         # Extract and print usernames\n",
    "#         for user in user_elements:\n",
    "#             usernames.add(user[\"href\"][1:-1])\n",
    "#     else:\n",
    "#         print(\"failed iter {}\", i)\n",
    "#         failed_fetch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f6241",
   "metadata": {},
   "source": [
    "## Scrape review data\n",
    "- scraping 1 users reviews took roughly 1 min\n",
    "- url doesnt work past 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "667c5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_review_to_json(file_path, user, film_title, unix_timestamp, rating, liked, review_text):\n",
    "    # Create a dictionary with the extracted details\n",
    "    review_data = {\n",
    "        \"user\": user,\n",
    "        \"title\": film_title,\n",
    "        \"review_date\": unix_timestamp,\n",
    "        \"rating\": rating,\n",
    "        \"liked\": liked,\n",
    "        \"review_text\": review_text\n",
    "    }\n",
    "    \n",
    "    # Load existing data if the file exists, else start with an empty list\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError:\n",
    "                data = []  # If the file is empty or corrupted, reset to an empty list\n",
    "    else:\n",
    "        data = []\n",
    "    \n",
    "    # Append new data to the list\n",
    "    data.append(review_data)\n",
    "\n",
    "    # Save the updated data back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "809da726",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in usernames:\n",
    "    url = \"https://letterboxd.com/{}/films/reviews/\".format(user)\n",
    "    \n",
    "    # Fetch the webpage\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        last_page = max(int(a.get_text()) for a in soup.select(\".paginate-page a\") if a.get_text().isdigit())\n",
    "\n",
    "        for i in range(1, last_page + 1):\n",
    "            url = \"https://letterboxd.com/{}/films/reviews/page/{}\".format(user, i)\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find all film detail blocks\n",
    "            film_blocks = soup.find_all(\"div\", class_=\"film-detail-content\")\n",
    "            # print(user, url)\n",
    "            # Iterate over each block and extract the required information\n",
    "            for film in film_blocks:\n",
    "                # Extract film title\n",
    "                film_title_tag = film.find(\"h2\", class_=\"headline-2 prettify\").find(\"a\")\n",
    "                film_title = film_title_tag.text.strip() if film_title_tag else \"Unknown Title\"\n",
    "                # print(film_title)\n",
    "\n",
    "\n",
    "                # Extract star rating\n",
    "                rating_tag = film.find(\"span\", class_=\"rating\")\n",
    "                rating_text = rating_tag.text.strip() if rating_tag else None\n",
    "                if rating_text:\n",
    "                    rating = rating_text.count('\\u00BD')*0.5 + rating_text.count('\\u2605')\n",
    "\n",
    "                # Extract date\n",
    "                date_tag = film.find(\"span\", class_=\"_nobr\")\n",
    "                date_string = date_tag.text.strip()\n",
    "                date_object = datetime.strptime(date_string, \"%d %b %Y\")\n",
    "                unix_timestamp = int(date_object.timestamp())\n",
    "\n",
    "                # Check if \"icon-liked\" exists in the attribution block\n",
    "                liked_icon = film.find(\"span\", class_=\"has-icon icon-16 icon-liked\")\n",
    "                liked = 1 if liked_icon else 0\n",
    "\n",
    "                # Extract review text\n",
    "                review_body_tag = film.find(\"div\", class_=\"body-text -prose js-review-body js-collapsible-text\")\n",
    "                review_text = review_body_tag.get_text(\" \", strip=True) if review_body_tag else None\n",
    "\n",
    "                # save review to json\n",
    "                save_review_to_json(\"../letterboxd_proj_data/test_reviews.json\", user, film_title, unix_timestamp, rating, liked, review_text)\n",
    "    else:\n",
    "        print(\"failed iter {}\", i)\n",
    "        failed_fetch += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd66459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
