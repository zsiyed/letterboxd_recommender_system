{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65efcca3-c18f-4e92-807b-a57796d8acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import json\n",
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "from surprise import *\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import GridSearchCV\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae48d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews\n",
    "def load_reviews_surprise(filename):\n",
    "    # read jsonl file and convert to pandas dataframe\n",
    "    reviews = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            reviews.append(json.loads(line))\n",
    "    ratings_df = pd.DataFrame(reviews)\n",
    "    ratings_df = ratings_df.dropna()  # Remove rows with NaN values\n",
    "    # Define a Surprise Reader object\n",
    "    reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "    # Load dataset into Surprise format\n",
    "    data = Dataset.load_from_df(ratings_df[['user', 'title', 'rating']], reader)\n",
    "    return data\n",
    "\n",
    "def load_reviews_pandas(filename):\n",
    "    # read jsonl file and convert to pandas dataframe\n",
    "    reviews = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            reviews.append(json.loads(line))\n",
    "    ratings_df = pd.DataFrame(reviews)\n",
    "    ratings_df = ratings_df.dropna()  # Remove rows with NaN values\n",
    "\n",
    "    return ratings_df[['user', 'title', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd79f7",
   "metadata": {},
   "source": [
    "# Load in small (100) review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc9c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "filename = '../letterboxd_proj_data/usernames_sample_100.jsonl'\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7acb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.DataFrame(reviews)\n",
    "ratings_df = ratings_df.dropna()  # Remove rows with NaN values\n",
    "# Define a Surprise Reader object\n",
    "reader = Reader(rating_scale=(0, 5))\n",
    "\n",
    "# Load dataset into Surprise format\n",
    "data = Dataset.load_from_df(ratings_df[['user', 'title', 'rating']], reader)\n",
    "\n",
    "# Create a train-test split\n",
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6200ef7e",
   "metadata": {},
   "source": [
    "# Trying different surprise lib models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "413b0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SVD(), NMF(), SlopeOne(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), KNNBaseline()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4767da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     print(model)\n",
    "    \n",
    "#     trainset = data.build_full_trainset()\n",
    "#     start = time.time()\n",
    "#     model.fit(trainset)\n",
    "#     print(time.time()-start)\n",
    "\n",
    "#     # Prepare testset for predictions\n",
    "#     testset = trainset.build_testset()\n",
    "#     predictions = model.test(testset)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df395644",
   "metadata": {},
   "source": [
    "# 2 step param search with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51502961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rmse': {'n_factors': 10, 'n_epochs': 100, 'lr_bi': 0.005, 'lr_bu': 0.005, 'biased': True}}\n",
      "Best RMSE: {'rmse': 0.8371488540008099}\n",
      "RMSE: 0.5699\n",
      "Test RMSE: 0.5698605988228671\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [5, 10],  # Number of latent factors\n",
    "    'n_epochs': [10, 50, 100],   # Number of epochs\n",
    "    'lr_bi': [0.002, 0.005],  # Learning rate\n",
    "    'lr_bu': [0.002, 0.005],  # Learning rate\n",
    "    # 'reg_pu': [0.01, 0.02, 0],  # Regularization term\n",
    "    # 'reg_qi': [0.01, 0.02, 0.1],  # Regularization term\n",
    "    # 'reg_bi': [0.01, 0.02, 0.1],  # Regularization term\n",
    "    'biased': [True],\n",
    "\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=5)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params)\n",
    "\n",
    "# Get the best RMSE score\n",
    "print(\"Best RMSE:\", grid_search.best_score)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator['rmse']\n",
    "\n",
    "# Optionally, you can fit the best model on the full training set and evaluate it on the test set\n",
    "trainset = data.build_full_trainset()\n",
    "best_model.fit(trainset)\n",
    "testset = trainset.build_testset()\n",
    "predictions = best_model.test(testset)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76486e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rmse': {'n_factors': 10, 'n_epochs': 200, 'lr_bi': 0.01, 'lr_bu': 0.005, 'biased': True}}\n",
      "Best RMSE: {'rmse': 0.8219330938202342}\n",
      "RMSE: 0.5383\n",
      "Test RMSE: 0.5382900734239495\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [10],  # Number of latent factors\n",
    "    'n_epochs': [100, 200],   # Number of epochs\n",
    "    'lr_bi': [0.005, 0.01, 0.1],  # Learning rate\n",
    "    'lr_bu': [0.005, 0.01, 0.1],  # Learning rate\n",
    "    # 'reg_pu': [0.01, 0.02, 0],  # Regularization term\n",
    "    # 'reg_qi': [0.01, 0.02, 0.1],  # Regularization term\n",
    "    # 'reg_bi': [0.01, 0.02, 0.1],  # Regularization term\n",
    "    'biased': [True],\n",
    "\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=5)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params)\n",
    "\n",
    "# Get the best RMSE score\n",
    "print(\"Best RMSE:\", grid_search.best_score)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator['rmse']\n",
    "\n",
    "# Optionally, you can fit the best model on the full training set and evaluate it on the test set\n",
    "trainset = data.build_full_trainset()\n",
    "best_model.fit(trainset)\n",
    "testset = trainset.build_testset()\n",
    "predictions = best_model.test(testset)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560f9b5",
   "metadata": {},
   "source": [
    "# NMF with best params with larger dataset\n",
    "- Best parameters: {'rmse': {'n_factors': 10, 'n_epochs': 200, 'lr_bi': 0.01, 'lr_bu': 0.005, 'biased': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "716cfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1000 = load_reviews('../letterboxd_proj_data/usernames_sample_100_1000.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be85c805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6724\n",
      "Test RMSE: 0.6723623926750961\n"
     ]
    }
   ],
   "source": [
    "NMF_model = NMF(n_factors = 10, n_epochs = 200, lr_bi = 0.01, lr_bu = 0.005, biased = True)\n",
    "# Optionally, you can fit the best model on the full training set and evaluate it on the test set\n",
    "trainset = data_1000.build_full_trainset()\n",
    "NMF_model.fit(trainset)\n",
    "testset = trainset.build_testset()\n",
    "predictions = NMF_model.test(testset)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7091b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetAutoFolds' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3929124/334753906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# linear regression with user and item means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_1000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetAutoFolds' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# linear regression with user and item means\n",
    "data_1000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec455a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
